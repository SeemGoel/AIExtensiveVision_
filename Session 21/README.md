### Assignment 21 
## Custom GPT-2 Text Generation with Hugging Face Integration(TheGiggleGrid)
This repository provides the tools and resources for training and using a custom text generation model based on the GPT-2 architecture. It draws inspiration from Andrey Karpathy's video walkthrough of GPT-2 code.

## Project Goal:

The primary aim of this project is to empower users to create and utilize a customized version of the GPT-2 model for text generation tasks. This involves training the model on a user-defined dataset. However some assignment objectives:
- On Colab (or your computer), train the 124M model on this such that your loss is less than 0.099999
- Share the GitHub link where we can see the training logs and sample outputs
- Share the huggingFace app where we can see it running (add a screenshot on GitHub where huggingface output is visible)


#### Key Features:

- Customizable GPT-2 Model: Implement the GPT-2 architecture from scratch or leverage existing implementations with tailored modifications.
- Comprehensive Training: Final Loss 0.03453453257679939 and tokens/sec:  2843.95 
- Dataset: Content of William Shakespeare plays(50304 tokens).
- Hugging Face Integration: Seamlessly integrate with the Hugging Face Transformers library for access to pre-trained models and further customization options.
Access it below
https://huggingface.co/spaces/SeemG/TheGiggleGrid




![image](https://github.com/SeemGoel/AIExtensiveVision_/assets/59606392/81fdf104-37be-4c83-bd78-c56ae949702a)
